{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwaDuQqCOyLJ"
      },
      "source": [
        "# **Tarea 4 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes: Gerard Cathalifaud Salazar, Juan Pablo Herrera Pizarro**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** Martes 13 de junio.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP).\n",
        "En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch.\n",
        "\n",
        "Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Auxiliar 3](https://youtu.be/36WTXvg3zh0) les puede ser de *gran ayuda*.\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
        "\n",
        "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)\n",
        "- [MEMMs and CRFs](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CRF.pdf): [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
        "- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf): [video](https://youtu.be/lLZW5Fn40r8)\n",
        "- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf): [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANqzQ3G9WNw3"
      },
      "source": [
        "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF) (1,5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXD3D7RYKJ-"
      },
      "source": [
        "### Pregunta 1 (1 pt)\n",
        "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
        "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
        "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "e(the|\\text{ DET}) &= 0.5 \\\\\n",
        "e(pasta|\\text{ NOUN}) &= 0.6\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
        "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
        "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
        "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EzgysW9kGi-"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Se desea determinar:\n",
        "\n",
        "$\\pi (8,\\text{DET},\\text{NOUN})$\n",
        "\n",
        "Seg√∫n el algoritmo de Viterbi, que se aprovecha de una definici√≥n recursiva de $\\pi$, se tiene que:\n",
        "\n",
        "$\\pi (k,u,v) = \\max_{w\\in S_{k-2}}(\\pi(k-1,w,u)\\times q(v|w,u)\\times e(x_k|v))$\n",
        "\n",
        "Entonces el c√°lculo se resumen en:\n",
        "\n",
        "$\\pi (8,\\text{DET},\\text{NOUN}) = \\max_{w\\in S}(\\pi(7,w,\\text{DET})\\times q(\\text{NOUN}|w,\\text{DET})\\times e(\\text{pasta}|\\text{NOUN}))$\n",
        "\n",
        "Notar que la expresi√≥n $q(\\text{NOUN}|w,\\text{DET})=0,\\forall w\\in S, w \\neq \\text{VERB}$ y $q(\\text{NOUN}|\\text{VERB},\\text{DET})=0.3$, por lo que todas las expresiones con $w$ distinto de $\\text{VERB}$ ser√°n 0 y el m√°ximo se obtendr√° con $w=\\text{VERB}$\n",
        "\n",
        "$\\pi (8,\\text{DET},\\text{NOUN}) = \\pi(7,\\text{VERB},\\text{DET})\\times q(\\text{NOUN}|\\text{VERB},\\text{DET})\\times e(\\text{pasta}|\\text{NOUN})$\n",
        "\n",
        "$\\pi (8,\\text{DET},\\text{NOUN}) = 0.01 \\times 0.3 \\times 0.6$\n",
        "\n",
        "$\\pi (8,\\text{DET},\\text{NOUN}) = 0.0018$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwJb_vmkKLZ"
      },
      "source": [
        "### Pregunta 2 (0.5 pts)\n",
        "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
        "\n",
        "#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Estos modelos buscan resolver las tareas de taggings, es decir, dada una secuencia de entrada (en PLN es un documento) entregar una etiqueta para cada una de estas entradas (palabras), como ejemplo se tienen los cl√°sicos problemas de:\n",
        "\n",
        "*   part-of-speech tagging (POS): Que consiste en categorizar gramaticalmente cada palabra en su funci√≥n del lenguaje.\n",
        "*   Named Entity Recognition (NER): Reconocer que palabras se refieren a alguna entidad (y a qu√© tipo) y cuales no.\n",
        "\n",
        "\n",
        "\n",
        "#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Los modelos MEMMs y CRFs presentan vectores de features. Este tiene la ventaja de realizar una representaci√≥n m√°s completa que el modelo HMMs que no lo utiliza, pues le agrega sensibilidad a partes espec√≠ficas del texto, como la presencia de bigramas, la existencia de prefijos o sufijos en la palabra, o la presencia de palabras en todo el texto, reflejando mayor informaci√≥n para resolver problemas en PLN.\n",
        "\n",
        "#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Si existe baja frecuencia de palabras en el set de entrenamiento, los modelos act√∫an de la siguiente forma:\n",
        "\n",
        "1.   HMMs:\n",
        "Divide el vocabulario en palabras frecuentes (>5 por ej) y palabras poco frecuentes (el resto), y a este √∫ltimo lo mapea a un conjunto finito peque√±o que los agrupe para as√≠ aumentar su frecuencia y procesarlos.\n",
        "\n",
        "2.   MEMMs y CRFs:\n",
        "Como utilizan un c√°lculo similar al softmax multiclase, se puede utilizar tambi√©n estrategias como suavizado de Laplace, que consiste en sumar 1 a todas las frecuencias, garantizando que toda probabilidad sea distinta de 0 y que no haya indeterminaciones. Tambi√©n se pueden aplicar features que otorguen informaci√≥n adicional a las palabras poco frecuentes, como la presencia de prefijos/sufijos.\n",
        "\n",
        "#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "CRF utiliza un vector de features que toma toda la sencuencia de entrada junto a toda una secuencia de etiquetas y normaliza por el espacio de todas las secuencias posibles, generando as√≠ una cobertura global para las decisiones. Por otro lado, MEMMs los va tomando localmente por cada paso de secuencia, en la posici√≥n i-√©sima. Los HMMs utilizan un supuesto de Markov, solo teniendo en cuenta las √∫ltimas 2 etiquetas anteriores, por lo que tambi√©n solo considera informaci√≥n a nivel local.\n",
        "\n",
        "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Para una secuencia de $m$ palabras, el conjunto de posibles secuencias de etiquetas es $S^m$ y tiene tama√±o $k^m$. Esto es de orden exponencial respecto a $m$ por lo que es computacionalmente intratable analizar todas las posibles secuencias. Sin embargo, mediante supuestos apropiados y algoritmos que los aprovechen como Viterbi, es posible tener entrenamientos y decoding eficientes con este conjunto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ACHHZIWGF1"
      },
      "source": [
        "# Convolutional Neural Networks (0,5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRAHR95Y8aB"
      },
      "source": [
        "### Pregunta 3 (0,5 puntos)\n",
        "\n",
        "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
        "\n",
        "La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n",
        "\\begin{equation}\n",
        "E = \\begin{pmatrix}\n",
        "2 & 2\\\\\n",
        "0 & -2\\\\\n",
        "0 & 1\\\\\n",
        "-2 & 1\\\\\n",
        "1 & 0\\\\\n",
        "-1 & 1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Los siguientes 3 filtros\n",
        "\\begin{equation}\n",
        "U = \\begin{pmatrix}\n",
        "-1 & 1 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 0 & -1\\\\\n",
        "1 & -1 & -1\\\\\n",
        "-1 & -1 & 1\\\\\n",
        "1 & 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Y la funci√≥n de activaci√≥n\n",
        "\\begin{equation}\n",
        "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
        "\\end{equation}\n",
        "\n",
        "Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQ30Arkq0u4"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Para aplicar la operaci√≥n de convoluci√≥n, es necesario definir una serie de ventanas de tama√±o $k$ que contengan los vectores de embedding de cada palabra, multiplicada por los filtros, al no tener padding, la primera ventana comenzar√° con la palabra `El`.\n",
        "\n",
        "Como cada palabra su vector de embedding es de dimensi√≥n $2$. Y la matriz de filtro tiene una dimensi√≥n de $6\\times 3$, por tanto, para que se pueda aplicar la matriz correctamente en cada ventana, se deber√° tener ventanas de tama√±o $3$, otorg√°ndole un tama√±o final de $6$ al concatenar los vectores de embedding.\n",
        "\n",
        "\\begin{equation}\n",
        "V_1 = {\\text{El, agua, moja}} = \\begin{pmatrix}\n",
        "2 & 2 & 0 & -2 & 0 & 1\n",
        "\\end{pmatrix}\\\\\n",
        "V_2 = {\\text{agua, moja, y}} = \\begin{pmatrix}\n",
        "0 & -2 & 0 & 1 & -2 & 1\n",
        "\\end{pmatrix}\\\\\n",
        "V_3 = {\\text{moja, y, el}} = \\begin{pmatrix}\n",
        "0 & 1 & -2 & 1 & 1 & 0\n",
        "\\end{pmatrix}\\\\\n",
        "V_4 = {\\text{y, el, fuego}} = \\begin{pmatrix}\n",
        "-2 & 1 & 1 & 0 & -1 & 1\n",
        "\\end{pmatrix}\\\\\n",
        "V_5 = {\\text{el, fuego, quema}} = \\begin{pmatrix}\n",
        "1 & 0 & -1 & 1 & 1 & 1\n",
        "\\end{pmatrix}\\\\\n",
        "\\end{equation}\n",
        "\n",
        "Cada una de estas ventanas, es multiplicada con el filtro y luego aplicada la funci√≥n de activaci√≥n `tanh`\n",
        "\n",
        "$V_1 \\times U = \\begin{pmatrix} -1 & 6 & 1 \\end{pmatrix} \\to tanh(V_1\\times U) = \\begin{pmatrix} tanh(-1) & tanh(6) & tanh(1) \\end{pmatrix} = \\begin{pmatrix} -0.76158416 & 0.99998771 & 0.76159416 \\end{pmatrix}$\n",
        "\n",
        "$tanh(V_2\\times U) = \\begin{pmatrix} 0.96402758 & -0.76159416 & -0.9993293 \\end{pmatrix}$\n",
        "\n",
        "$tanh(V_3\\times U) = \\begin{pmatrix} 0.76159416 & -0.76159416 & 0.96402758 \\end{pmatrix}$\n",
        "\n",
        "$tanh(V_4\\times U) = \\begin{pmatrix} 0.9999092 & 0. & -0.99505475 \\end{pmatrix}$\n",
        "\n",
        "$tanh(V_5\\times U) = \\begin{pmatrix} 0. & -0.76159416 & 0. \\end{pmatrix}$\n",
        "\n",
        "Finalmente, se aplica el max pooling, que consiste en tomar el valor m√°ximo de cada dimensi√≥n formando un nuevo vector 3D, el resultado entregado:\n",
        "\n",
        "\\begin{pmatrix} 0.9999092 & 0.99998771 & 0.96402758 \\end{pmatrix}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdvL6LanJByf",
        "outputId": "bbd59dee-0bd1-43ce-b69c-2199eebe6fdf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.76159416  0.99998771  0.76159416]]\n",
            "[[ 0.96402758 -0.76159416 -0.9993293 ]]\n",
            "[[ 0.76159416 -0.76159416  0.96402758]]\n",
            "[[ 0.9999092   0.         -0.99505475]]\n",
            "[[ 0.         -0.76159416  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "U = np.array([[-1,1,0],[1,1,0],[0,0,-1],[1,-1,-1],[-1,-1,1],[1,0,-1]])\n",
        "\n",
        "v1 = np.array([[2,2,0,-2,0,1]])\n",
        "v2 = np.array([[0,-2,0,1,-2,1]])\n",
        "v3 = np.array([[0,1,-2,1,1,0]])\n",
        "v4 = np.array([[-2,1,1,0,-1,1]])\n",
        "v5 = np.array([[1,0,-1,1,1,1]])\n",
        "\n",
        "\n",
        "\n",
        "r1 = np.matmul(v1,U)\n",
        "r2 = np.matmul(v2,U)\n",
        "r3 = np.matmul(v3,U)\n",
        "r4 = np.matmul(v4,U)\n",
        "r5 = np.matmul(v5,U)\n",
        "\n",
        "print(np.tanh(r1))\n",
        "print(np.tanh(r2))\n",
        "print(np.tanh(r3))\n",
        "print(np.tanh(r4))\n",
        "print(np.tanh(r5))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0rCwen3WREC"
      },
      "source": [
        "# Recurrent Neural Networks (1 punto)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0et78Z4oKIq"
      },
      "source": [
        "### Pregunta 4 (0,5 puntos)\n",
        "Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
        "\n",
        "Tenemos una red recurrente *Elman* definidad como:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
        "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "donde\n",
        "\\begin{equation}\n",
        "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
        "\\end{equation}\n",
        "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
        "\n",
        "Sea\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_0 &= [0,0,0]\\\\\n",
        "W^x &= \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} \\\\\n",
        "W^s &= \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix} \\\\\n",
        "\\vec{b} &= [0, 0, 0] \\\\\n",
        "g(x) &= ReLu(x) = max(0, x)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fim2W8JioPhL"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Se calculan los vectores secuencialmente:\n",
        "\n",
        "$\\vec{s}_1 = R_{SRNN}(\\vec{x}_1,\\vec{s}_0) = g(\\vec{s}_0W^s + \\vec{x}_1W^x+\\vec{b}) = g([0,0,0]\\begin{pmatrix}1 & 0 & 1\\\\ 0 & 1 & -1\\\\ 1 & 1 & 1\\end{pmatrix} + [1, 0] \\begin{pmatrix}0 & 0 & 1\\\\ 1 & -1 & 0\\end{pmatrix} + [0,0,0]) = g([0,0,0] + [0,0,1]) = g([0,0,1]) = [0,0,1]$\n",
        "\n",
        "$\\vec{s}_2 = R_{SRNN}(\\vec{x}_2,\\vec{s}_1) = g(\\vec{s}_1W^s + \\vec{x}_2W^x+\\vec{b}) = g([0,0,1]\\begin{pmatrix}1 & 0 & 1\\\\ 0 & 1 & -1\\\\ 1 & 1 & 1\\end{pmatrix} + [-1, 1] \\begin{pmatrix}0 & 0 & 1\\\\ 1 & -1 & 0\\end{pmatrix} + [0,0,0]) = g([1,1,1] + [1,-1,-1]) = g([2,0,0]) = [2,0,0]$\n",
        "\n",
        "$\\vec{s}_3 = R_{SRNN}(\\vec{x}_3,\\vec{s}_2) = g(\\vec{s}_2W^s + \\vec{x}_3W^x+\\vec{b}) = g([2,0,0]\\begin{pmatrix}1 & 0 & 1\\\\ 0 & 1 & -1\\\\ 1 & 1 & 1\\end{pmatrix} + [1, 1] \\begin{pmatrix}0 & 0 & 1\\\\ 1 & -1 & 0\\end{pmatrix} + [0,0,0]) = g([2,0,2] + [1,-1,1]) = g([3,-1,3]) = [3,0,3]$\n",
        "\n",
        "\n",
        "Luego, simplemente $\\vec{y}_i = \\vec{s}_i, \\forall i \\in \\{1,2,3\\}$, entonces los resultados finales son:\n",
        "\n",
        "$\\vec{y}_1 = \\vec{s}_1 = [0,0,1]$\n",
        "\n",
        "$\\vec{y}_2 = \\vec{s}_2 = [2,0,0]$\n",
        "\n",
        "$\\vec{y}_3 = \\vec{s}_3 = [3,0,3]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4rAT6ELxRZW"
      },
      "source": [
        "### Pregunta 5 (0.5 puntos)\n",
        "¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n",
        "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AXbQSgA_t8"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Las RNN y las CNN aprenden representaciones espec√≠ficas para la tarea objetivo a trav√©s de su capacidad para capturar dependencias a largo plazo en secuencias (RNN) o extraer caracter√≠sticas relevantes localmente en datos estructurados (CNN). En contraste con los modelos que utilizan caracter√≠sticas dise√±adas manualmente, las RNN y las CNN aprenden autom√°ticamente representaciones directamente de los datos, lo que les permite adaptarse mejor a la complejidad y variabilidad de los datos y obtener un mejor rendimiento en tareas desafiantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRJkBpjWyHnb"
      },
      "source": [
        "# Pregunta 6: Redes Neuronales con Pytorch (3 puntos) üí¨\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.anda.cl/wp-content/uploads/2021/03/0_5vNAtimPjYQr4W72.gif\" alt=\"chatbot\" width=\"400\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEla92bUymrQ"
      },
      "source": [
        "En esta secci√≥n de la tarea deber√°n implementar un Chatbot que sea capaz de generar una conversaci√≥n *‚Äúb√°sica‚Äù* utilizando un dataset de *Star Wars*. **El objetivo** de esta pregunta es que puedan aplicar lo aprendido sobre redes neuronales utilizando Pytorch en un ejemplo pr√°ctico.  Durante el desarrollo, se espera que puedan dise√±ar un bot (que tendr√° por atr√°s un clasificador) que sea capaz de clasificar diferentes etiquetas, cosa que una vez identificada la etiqueta entregue una respuesta acorde a lo preguntado.\n",
        "\n",
        "**Aviso:** Antes de comenzar con una descripci√≥n mas profunda de esta secci√≥n, les recomendamos que visualicen y se familiaricen con el dataset entregado, de esta forma comprender√°n mejor la descripci√≥n del enunciado (aqu√≠ una peque√±a ayudita üÜò)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKOGlMs3Dx-",
        "outputId": "f12ec764-0216-4660-bacb-d13e4acd17b2",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tags:  16\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_data = pd.read_json('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json')\n",
        "print(\"Cantidad de tags: \", example_data['intents'].shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-6fCE5fHkNS"
      },
      "source": [
        "A continuaci√≥n, ejemplos del contenido del primer registro:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axsi27BpHGOx",
        "outputId": "58f4eef6-53d0-4958-bc05-d2c9eb3a2414",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi',\n",
              " 'Hey',\n",
              " 'How are you',\n",
              " 'Is anyone there?',\n",
              " 'Hello',\n",
              " 'Good day',\n",
              " \"What's up\",\n",
              " 'Yo!',\n",
              " 'Howdy',\n",
              " 'Nice to meet you.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "example_data['intents'][0]['patterns']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV0vGdwoHeg3",
        "outputId": "4f89701d-480b-4f80-d1b4-9a9c4e4ab277",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hey',\n",
              " 'Hello, thanks for visiting.',\n",
              " 'Hi there, what can I do for you?',\n",
              " 'Hi there, how can I help?',\n",
              " 'Hello, there.',\n",
              " 'Hello Dear',\n",
              " 'Ooooo Hello, looking for someone or something?',\n",
              " 'Yes, I am here.',\n",
              " 'Listening carefully.',\n",
              " 'Ok, I am with you.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example_data['intents'][0]['responses']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0BnYez1oGtx3",
        "outputId": "6a048ff4-b040-43b8-f480-1a77e93b4a0b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'greeting'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "example_data['intents'][0]['tag']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6BvAWCw3zPM"
      },
      "source": [
        "Del dataset cargado podemos notar que este viene en un formato `JSON`, por lo que sus datos est√°n almacenados en diccionarios. Las llaves de los diccionarios no son aleatorias y estos nos sirven para identificar puntos relevantes en el desarrollo del bot. A continuaci√≥n, se realiza una peque√±a descripci√≥n de las llaves:\n",
        "\n",
        "- `patterns`: Almacena los patrones con los que entrenaremos el modelo üòÆ, en otras palabras, es el corpus de entrenamiento que contiene solo preguntas o expresiones que deber√° responder el bot.\n",
        "- `responses`: Son las respuestas üôã relacionadas a los `patterns`, estas las utilizaremos en una etapa posterior a la clasificaci√≥n, para dar una respuesta aleator√≠a al usuario.\n",
        "- `tag`: Son las labels con las que entrenaremos nuestro modelo üíª.\n",
        "\n",
        "En s√≠ntesis, las `keys` relevantes para el entrenamiento de nuestra red neuronal ser√°n `patterns` (corpus) y `tag` (etiquetas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlOAdMjSSzNN"
      },
      "source": [
        "#### Explicaci√≥n de la tarea a realizar:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yGApnWVI4cO"
      },
      "source": [
        "**Explicaci√≥n de la tarea a realizar**: Implemente una Class llamada `CNNClassifier` que sea capaz de entrenar un modelo de texto a trav√©s de una red neuronal Feed Forward y una arquitectura convolucional (CNN 1D) [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) . Para el dise√±o de las redes tienen completa libertad, pero se le aconseja que se gu√≠en de la √∫ltima auxiliar para la construcci√≥n. Es **important√≠simo** que el modelo a crear posea una capa de `Embedding` que se genere en base al entrenamiento del modelo. Creado el modelo, construya una funci√≥n batch para cargar los datos de entrenamiento del modelo.\n",
        "\n",
        "Construido el modelo, compare los resultados obtenidos para una red feed forward y una cnn. Para la comprobaci√≥n de sus resultados ejecute el chatbot y pruebelo, ¬øqu√© configuraci√≥n tiene mejores resultados?, ¬øa qu√© se deberan estos resultados?\n",
        "\n",
        "Ojo que un ejemplo de prueba con el chatbot puede ser (agregue mas preguntas ud):\n",
        "\n",
        "```\n",
        "Let's chat! (type 'finish_chat' to finish the chat)\n",
        "You: hi\n",
        "GA-97: Yes, I am here.\n",
        "You: can you tell me a joke?\n",
        "GA-97: Have you tried the gluten-free Wookiee treats? No, but I heard they are a little Chewy.\n",
        "```\n",
        "\n",
        "El resto del c√≥digo referido a la ejecuci√≥n del chat se los entregamos, por lo que no deber√≠an tener mayores problemas üò∏ (en caso de tener problemas con su c√≥digo, puede modificar cualquier parte sugerida siempre y cuando cumpla lo solicitado).\n",
        "\n",
        "**Igual [mucho texto](https://i0.wp.com/elgeneracionalpost.com/wp-content/uploads/2020/07/mucho-texto.jpg?fit=1280%2C720&ssl=1).... En resumen, ¬øQu√© se solicita?:**\n",
        "\n",
        "- [ ] Dise√±ar una red neuronal Feed Forward.\n",
        "- [ ] Dise√±ar un red convolucional.\n",
        "- [ ] Utilizar una capa de embeddings para generar representaciones vectoriales del corpus.\n",
        "- [ ] Crear el m√©todo forward de la clase `CNNClassifier`.\n",
        "- [ ] Crear la funci√≥n BATCH.\n",
        "- [ ] Probar el modelo y comparar los resultados obtenidos con la red Feed Forward y la red CNN. Comente sus resultados de forma cualitativa, se√±alando con qu√© tipo de red obtuvo mejores resultados con el chatbot.\n",
        "\n",
        "**Nota-1:** El modelo creado debe tener la opci√≥n de entrenar a traves de una feed forward y una CNN. Esto no significa que entrenar√° una FF y una CN, el modelo deber√° recibir un booleano que especifique que tipo de red utilizar√°.\n",
        "\n",
        "**Nota-2:** El dataset se descargar√° autom√°ticamente en la secci√≥n `Carga de Dataset üìö`, no os preocup√©is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4bKfAdEy3oD"
      },
      "source": [
        "#### Pasemos al C√≥digo ü¶æ\n",
        "\n",
        "Esqueleto propuesto (se **RECOMIENDA** que cambien **SOLO** la red neuronal y la funci√≥n Batch) ü¶¥:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUwxivx2MpMV"
      },
      "source": [
        "##### Instalamos librerias necesarias e importamos üòÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjSZkBsk1H4f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Esto toma su tiempo en ejecutarse\n",
        "%%capture\n",
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchtext==0.9.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfZ6SL-Q1Kwd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from random import choice\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj-Epe7XJLrL"
      },
      "source": [
        "##### Carga de Dataset üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvlLqYRrVN6l",
        "outputId": "bd63ef94-d05a-423d-98f8-bef7439b92eb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-18 23:10:23--  https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14469 (14K) [text/plain]\n",
            "Saving to: ‚Äòstar_wars_chatbot.json‚Äô\n",
            "\n",
            "\rstar_wars_chatbot.j   0%[                    ]       0  --.-KB/s               \rstar_wars_chatbot.j 100%[===================>]  14.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-06-18 23:10:23 (39.6 MB/s) - ‚Äòstar_wars_chatbot.json‚Äô saved [14469/14469]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# we obtain the dataset\n",
        "!wget 'https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbbIsFUG1TXW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load the dataset using json\n",
        "with open('star_wars_chatbot.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Create a vocab with the dataset and get the number of classes that have\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x) for list_words in dataset['intents'] for x in list_words['patterns'])\n",
        "num_classes = len(dataset['intents'])\n",
        "\n",
        "# Define a list with the labels\n",
        "labels = sorted(set([tag for tag in [intents['tag'] for intents in dataset['intents']]]))\n",
        "# Define a train_list where we can find the info in the format: [(tag_0, text_0)...,(tag_n-1, text_n-1)]\n",
        "train_list = [(labels.index(intents['tag']), text) for intents in dataset['intents'] for text in intents['patterns']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "\n",
        "labels = list({doc[0] for doc in train_list})\n",
        "label_map = {label: index for index, label in enumerate(labels)}\n",
        "#creamos lista de tensores\n",
        "train_dataset = [\n",
        "        (\n",
        "            item[0], #numero de la clase\n",
        "            torch.tensor([vocab[token] for token in tokenizer(item[1])]),\n",
        "        ) for item in train_list\n",
        "    ]\n",
        "\n",
        "#train_dataset = list(zip(labels,\n",
        "#                    torch.tensor([item[1] for item in train_list])))\n"
      ],
      "metadata": {
        "id": "zsvKSDJbfesZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_list[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlSRw_quEYlb",
        "outputId": "b93f35b3-4d84-41e6-9b9d-09c51c3f7181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(7, 'Hi'), (7, 'Hey'), (7, 'How are you'), (7, 'Is anyone there?'), (7, 'Hello')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-f9kGQMD57L",
        "outputId": "a19999d2-a349-4b11-f5c6-a7816ca48a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(7, tensor([93])), (7, tensor([92])), (7, tensor([95, 20,  3])), (7, tensor([ 12,  71, 121,   2])), (7, tensor([90]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JDkdqIIBvLs",
        "outputId": "7c22b4ef-ef43-4b7c-8f9e-6deae72047fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_text = ''\n",
        "for i in range(20):\n",
        "  human_text += vocab.get_itos()[i] + ' '\n",
        "print(f\"text: {human_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3vYk0Hffrz3",
        "outputId": "0fc844d5-a6e0-4fba-e54a-eb69d8c04a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: . ? you me i help in ! can a who is need tell am do what for mission are \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a52SUNKPJQxi"
      },
      "source": [
        "##### Creaci√≥n del modelo (2 puntos en total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD6o1hMCRnFG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "UNK_IDX = 0\n",
        "vocab.set_default_index(UNK_IDX)\n",
        "\n",
        "vocab.insert_token('<pad>', 1)\n",
        "\n",
        "num_class = len(labels) # Se puede borrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-vQ24tMJG5H",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Construya el modelo\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10,\n",
        "                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=1):\n",
        "      super().__init__()\n",
        "      # Creamos la capa de embedding\n",
        "      self.use_cnn = use_cnn\n",
        "      if self.use_cnn:\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Creamos la capa de convoluci√≥n\n",
        "        # `in_channels`: Es el n√∫mero de canales de entrada de la convoluci√≥n. En este caso, como estamos trabajando con texto, s√≥lo tenemos un canal, por lo que `in_channels=1`.\n",
        "        # `out_channels`: Es el n√∫mero de canales de salida de la convoluci√≥n. Especifica la cantidad de filtros que se aplicar√°n a la entrada. En este caso, queremos generar `cnn_pool_channels` canales de salida, por lo que `out_channels=cnn_pool_channels`.\n",
        "        # `kernel_size`: Es el tama√±o del kernel de la convoluci√≥n. En este caso, estamos usando un kernel de tama√±o `cnn_kernel_size * embed_dim`, donde `embed_dim` es la dimensi√≥n de los vectores de embedding. Esto significa que cada filtro de la convoluci√≥n cubrir√° `cnn_kernel_size` palabras (o tokens) en una dimensi√≥n y `embed_dim` en la otra.\n",
        "        # `stride`: Es el desplazamiento que se aplica a la entrada de la convoluci√≥n. En este caso, estamos desplazando la entrada `embed_dim` unidades en cada paso. Esto significa que se aplicar√°n filtros a cada palabra (o token) de la entrada.\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=cnn_pool_channels,\n",
        "            kernel_size=cnn_kernel_size * embed_dim,\n",
        "            stride=embed_dim,\n",
        "        )\n",
        "\n",
        "        # Calculamos el tama√±o de entrada de la capa lineal\n",
        "        fc_in_size = cnn_pool_channels\n",
        "\n",
        "        # Creamos la capa lineal\n",
        "        self.fc = nn.Linear(fc_in_size, num_classes)\n",
        "\n",
        "        # Inicializamos los pesos de las capas\n",
        "        self.init_weights()\n",
        "      else:\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, 1) # 1 el √≠ndice de pad\n",
        "\n",
        "        # capas de la MLP\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def init_weights(self):\n",
        "      # Definimos el rango de los valores iniciales de los pesos\n",
        "      initrange = 0.5\n",
        "\n",
        "      # Inicializamos los pesos de la capa de embedding\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "      # Inicializamos los pesos de la capa lineal\n",
        "      self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "      # Inicializamos los sesgos de la capa lineal en cero\n",
        "      self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "\n",
        "      # Preparamos el input de la capa de embeddings a partir de text y offsets\n",
        "      text = torch.tensor(\n",
        "          list(\n",
        "              zip(\n",
        "                  *zip_longest(\n",
        "                      *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]),\n",
        "                      fillvalue=vocab[\"<pad>\"]\n",
        "                  )\n",
        "              )\n",
        "          )\n",
        "      ).to(text.device)\n",
        "\n",
        "      # Obtenemos la representaci√≥n de la frase a partir de la capa de embedding\n",
        "      h = self.embedding(text)\n",
        "\n",
        "      # Aplicamos la capa de convoluci√≥n\n",
        "      h = h.view(h.size(0), 1, -1)\n",
        "      h = torch.relu(self.conv(h))\n",
        "      h = h.mean(dim=2)\n",
        "\n",
        "      # Obtenemos el resultado final a partir de la capa lineal\n",
        "      output = self.fc(h)\n",
        "\n",
        "      # Aplicamos la funci√≥n de activaci√≥n log-softmax\n",
        "      return nn.functional.log_softmax(output, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ArgumentClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size, pad_idx):\n",
        "        super().__init__()\n",
        "\n",
        "        # capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, pad_idx) # 1 x 100\n",
        "        # self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n",
        "\n",
        "        # capas de la MLP\n",
        "        self.fc = nn.Linear(embed_dim, num_class) # 100 x 16\n",
        "        # self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
        "        # self.fc2 = nn.Linear(hidden_size, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # La representacion de un documento sera el promedio de los\n",
        "        # embeddings de sus palabras.\n",
        "        # (B, N, 1) -> (B, N, E)\n",
        "        # print(f\"batch: {batch.shape}\")\n",
        "        h = self.embedding(batch)\n",
        "        # print(f\"h1: {h.shape}\")\n",
        "        # (B, N, E) -> (B, E)\n",
        "        h = h.mean(dim=1)\n",
        "        # print(f\"h2: {h.shape}\")\n",
        "        # h = self.embedding(batch)\n",
        "\n",
        "        # computar las capas de la red MLP\n",
        "        h = self.fc(h)\n",
        "        # print(f\"h3 {h.shape}\")\n",
        "        # h = F.relu(self.fc1(h))\n",
        "        # h = self.fc2(h)\n",
        "\n",
        "        return h\n",
        "        # return torch.softmax(h, -1)"
      ],
      "metadata": {
        "id": "lmo056fZ-5i9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGN-T0JoJtmS"
      },
      "source": [
        "##### Funci√≥n Batch üë∑ (0,5 puntos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1AZpXc7JxTa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Defina su funci√≥n de BATCH\n",
        "# output:  (label, texts)\n",
        "def generate_batch_forward(batch):\n",
        "  return (\n",
        "        # En este caso como los labels son n√∫meros,\n",
        "        # el tensor es de una sola dimension de tama√±o batch_size\n",
        "        torch.tensor([item[0] for item in batch]),\n",
        "\n",
        "        # En este caso se retorna un tensor de 2 dimensiones, batch_size x N,\n",
        "        # donde N es mayor largo de los ejemplo en el batch. Aca se realiza\n",
        "        # padding de los ejemplos mas cortos.\n",
        "        torch.tensor(\n",
        "            list(\n",
        "                zip(\n",
        "                    *zip_longest(\n",
        "                        *[item[1] for item in batch], fillvalue=vocab[\"<pad>\"]\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "\n",
        "# output: (texts, offsets, label)\n",
        "def generate_batch_CNN(batch):\n",
        "  stoi = vocab.get_stoi()\n",
        "  label = torch.tensor([entry[0] for entry in batch]) # Ac√° es distinto del aux\n",
        "  texts = [tokenizer(entry[1]) for entry in batch]\n",
        "  offsets = [0] + [len(text) for text in texts]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  big_text = torch.cat([torch.tensor([vocab[t] if t in stoi else 0 for t in text]) for text in texts])\n",
        "  #big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n",
        "\n",
        "  return big_text, offsets, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = generate_batch_forward(train_dataset[1:10])\n",
        "batch[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcqmcJRs-wDM",
        "outputId": "aedcd026-bed8-48d7-beba-f62fb1324f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 92,   1,   1,   1,   1],\n",
              "        [ 95,  20,   3,   1,   1],\n",
              "        [ 12,  71, 121,   2,   1],\n",
              "        [ 90,   1,   1,   1,   1],\n",
              "        [ 55,  52,   1,   1,   1],\n",
              "        [ 17,  38,  44, 126,   1],\n",
              "        [130,   8,   1,   1,   1],\n",
              "        [ 96,   1,   1,   1,   1],\n",
              "        [ 60,  65, 102,   3,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUVI6RX7S6ug",
        "outputId": "9dcfc262-94d3-4448-9483-507cfec911c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(7, tensor([92])),\n",
              " (7, tensor([95, 20,  3])),\n",
              " (7, tensor([ 12,  71, 121,   2])),\n",
              " (7, tensor([90])),\n",
              " (7, tensor([55, 52])),\n",
              " (7, tensor([ 17,  38,  44, 126])),\n",
              " (7, tensor([130,   8])),\n",
              " (7, tensor([96])),\n",
              " (7, tensor([ 60,  65, 102,   3,   0]))]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChwpNrrNRBe"
      },
      "source": [
        "##### Entrenamiento ü•ä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5eRWRD_J0Km",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = True\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=True).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch_CNN)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    #output = model(text) # Forward\n",
        "    output = model(texts, offsets) # CNN\n",
        "    #print(cls)\n",
        "    #print(cls+1)\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Feed Forward"
      ],
      "metadata": {
        "id": "wkkcQSOhFTbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = False\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=False).to(device)\n",
        "\n",
        "# model = ArgumentClassifier(\n",
        "#     vocab_size=len(vocab),\n",
        "#     embed_dim=100,\n",
        "#     num_class=num_classes,\n",
        "#     hidden_size=1024,\n",
        "#     pad_idx=vocab[\"<pad>\"],\n",
        "# ).to(device)\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_forward)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  enum = enumerate(train_loader)\n",
        "  #print(list(train_loader))\n",
        "  #continue\n",
        "\n",
        "  #for i, (cls, offsets, texts) in enumerate(train_loader):\n",
        "  for i, (cls, texts) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    #offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(texts) # Forward\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "tolQZX1x17vp",
        "outputId": "6b779015-f620-4851-8192-03a87b43907a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is avaible: cpu\n",
            "train: 97 elements\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-f88a13a88137>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CNNClassifier.forward() missing 1 required positional argument: 'offsets'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora creamos funciones para entrenar y validar el modelo\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def train_func(train_dataset):\n",
        "\n",
        "    # Entranamos el modelo\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=generate_batch_forward,\n",
        "    )\n",
        "    for i, (cls, text) in enumerate(data):\n",
        "        optimizer.zero_grad()\n",
        "        cls, text = cls.to(device), text.to(device)\n",
        "        output = model(text)\n",
        "\n",
        "        loss = criterion(output, cls)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Ajustar el learning rate\n",
        "    # scheduler.step()\n",
        "\n",
        "    return train_loss / len(train_dataset), train_acc / len(train_dataset)\n",
        "\n",
        "\n",
        "def test(test_dataset):\n",
        "    test_loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(\n",
        "        test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch_forward\n",
        "    )\n",
        "    for cls, text in data:\n",
        "        cls, text = cls.to(device), text.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(text)\n",
        "            loss = criterion(output, cls)\n",
        "            test_loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return test_loss / len(test_dataset), acc / len(test_dataset)"
      ],
      "metadata": {
        "id": "MiQU42zu8AuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora por fin tenemos todo lo necesario para entrenar el modelo.\n",
        "import time\n",
        "\n",
        "N_EPOCHS = 20\n",
        "LEARN_RATE = 1e-1\n",
        "STEP_SIZE = 1\n",
        "BATCH_SIZE = 16\n",
        "EMBED_DIM = 99\n",
        "HIDDEN_SIZE = 1024\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 2\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
        "model = ArgumentClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=EMBED_DIM,\n",
        "    num_class=num_classes,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    pad_idx=vocab[\"<pad>\"],\n",
        ").to(device)\n",
        "\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(train_dataset[3:])\n",
        "    #valid_loss, valid_acc = test(validation_dataset)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs // 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\",\n",
        "    )\n",
        "    print(\n",
        "        f\"\\tLoss: {train_loss:.4f}(train)\\t|\"\n",
        "        f\"\\tAcc: {train_acc * 100:.1f}%(train)\"\n",
        "    )\n",
        "    # print(\n",
        "    #     f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\"\n",
        "    #     f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\"\n",
        "    # )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "JFfzJczi8IOz",
        "outputId": "409e705a-b68e-43a6-b753-e3a5ef245b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch: torch.Size([16])\n",
            "h1: torch.Size([16, 99])\n",
            "h2: torch.Size([16])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-ba77e2d1650a>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m#valid_loss, valid_acc = test(validation_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-dbe3f9496319>\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(train_dataset)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-98-344ab4b73df6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# computar las capas de la red MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"h3 {h.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# h = F.relu(self.fc1(h))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x16 and 99x16)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axi_S_52IPex",
        "outputId": "c6c4c7a2-9b32-48b7-cab6-2b6e5baeed79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, tensor([95, 20,  3]))"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPdjcFJIK2sX",
        "outputId": "40edf385-8a24-41fe-de8c-6b3092413d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 'Hi')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I23EIfALGWSK",
        "outputId": "482a9510-1dfb-4310-fb04-c0c7aec5d696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, tensor([93]))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dlS4_X-L3DN"
      },
      "source": [
        "##### A probar! üß™"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IhhAKFXL3eH",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "43583a71-7410-4c49-da2c-2188bef64324"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'funny'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# This is working?, Try the next example!\n",
        "qText = \"'Do you know any joke?'\" # this must classify the label \"funny\"\n",
        "\n",
        "X = torch.tensor([vocab.get_stoi()[t] for t in tokenizer(qText)]).to(device)\n",
        "\n",
        "model.eval()\n",
        "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "labels[predicted]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TofTfwNDiggh",
        "outputId": "5ba019f3-1e57-4d08-f833-640bef050cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Menu', 'about me', 'alive', 'bounti hounter', 'creator', 'funny', 'goodbye', 'greeting', 'help', 'jedi', 'mission', 'myself', 'sith', 'stories', 'tasks', 'thanks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3TXD8raoDbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udemze3zL549"
      },
      "source": [
        "Ya pero prometiste hacer un chatbot, no una simple clasificaci√≥n...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpSYGx2tL0tC"
      },
      "source": [
        "##### Guardamos modelo ü¶∫ (opcional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBC4TyiqLzDv",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18e3ae83-899a-4dbb-f229-bb4e18b956c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ],
      "source": [
        "# We save de model using pytorch (this is optional, just to learn how to do this in pytorch)\n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": INPUT_SIZE,\n",
        "\"output_size\": OUTPUT_SIZE,\n",
        "\"use_cnn\": USE_CNN,\n",
        "\"labels\": labels\n",
        "        }\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYClbTtsMCjE"
      },
      "source": [
        "##### Chatbot üí¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c249zUwiMBxb",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "85d32bed-7885-4256-dd36-f6078fe7365b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'finish_chat' to finish the chat)\n",
            "You: hi\n",
            "tag: greeting\n",
            "GA-97: Hello, there.\n",
            "You: tell me a joke\n",
            "tag: bounti hounter\n",
            "GA-97: Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.\n",
            "You: tell\n",
            "tag: bounti hounter\n",
            "GA-97: I will advise you to look for Jango Fett, Boba Fett, Cad Bane, Durge, Embo, Dengar, Black Krrsantan, IG-88, Aurra Sing, Sabine Wren.\n",
            "You: joke\n",
            "tag: funny\n",
            "GA-97: What‚Äôs Yoda‚Äôs advice for going to the bathroom? Doo-doo or doo-doo-not-do.\n",
            "You: story\n",
            "tag: stories\n",
            "GA-97: You would get bored if I do so.\n",
            "You: bye\n",
            "tag: goodbye\n",
            "GA-97: Well, hope see you soon!\n",
            "You: hello\n",
            "tag: greeting\n",
            "GA-97: Hello Dear\n",
            "You: menu please\n",
            "tag: Menu\n",
            "GA-97: Menu: Fuzzy Tauntaun, Bloody Rancor, Jedi Mind Trick, T-16 Skyhopper, Yub Nub, Jet Juice, Hyperdrive, Rancor Beer.\n",
            "You: sdsdsfdsf\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-c9dc8007922a>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-132-c9dc8007922a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stoi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'sdsdsfdsf'"
          ]
        }
      ],
      "source": [
        "hdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('star_wars_chatbot.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "INPUT_SIZE = data[\"input_size\"]\n",
        "OUTPUT_SIZE = data[\"output_size\"]\n",
        "USE_CNN = data[\"use_cnn\"]\n",
        "labels = data['labels']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "# Dictionary with the answers\n",
        "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
        "\n",
        "bot_name = \"GA-97\"\n",
        "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
        "while True:\n",
        "    q_text = input(\"You: \")\n",
        "    q_text = q_text\n",
        "    if q_text == 'finish_chat':\n",
        "        break\n",
        "\n",
        "    X = torch.tensor([vocab.get_stoi()[t] for t in tokenizer(q_text)]).to(device)\n",
        "\n",
        "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = labels[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.50:\n",
        "      print(f\"tag: {tag}\")\n",
        "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
        "    else:\n",
        "      print(f\"{bot_name}: My model can't understand you...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Hu2QTuSURCt"
      },
      "source": [
        "#### Comente los resultados aqu√≠ (0,5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdFV63WVUX32"
      },
      "source": [
        "``Comente los resultados aqu√≠``"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}